\begin{thebibliography}{9}

\bibitem{li_rl}
Yu Li \\
\textit{IA et Machine Learning - Chapitre 2 : Apprentissage par Renforcement} \\
\url{https://home.mis.u-picardie.fr/~yli/docs/DdR-6/chap1.pdf}

\bibitem{rl_an_intro}
Richard S. Sutton and Andrew G. Barto\\
\textit{Reinforcement Learning: An Introduction}\\
\url{http://incompleteideas.net/book/the-book.html}

\bibitem{morvanzhou}
MorvanZhou \\
\url{https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow}

\bibitem{dl_mit}
Ian Goodfellow and Yoshua Bengio and Aaron Courville \\
\textit{Deep Learning} \\
\url{http://www.deeplearningbook.org/}

\bibitem{colah}
Christopher Olah \\
\textit{colah's blog} \\
\url{https://colah.github.io/}

\bibitem{3b1b_nn}
3Blue1Brown \\
\textit{Deep learning} \\
\url{https://youtu.be/aircAruvnKk}

\bibitem{stanford_drl} 
Fei-Fei Li, Justin Johnson, Serena Yeung \\
\textit{Lecture 14 | Deep Reinforcement Learning} \\
\url{https://www.youtube.com/watch?v=lvoHnicueoE}

\bibitem{keon}
Keon \\
\textit{Minimal Deep Q Learning (DQN \& DDQN) implementations in Keras} \\
\url{https://github.com/keon/deep-q-learning}

\bibitem{atari_drl}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller
\textit{Playing Atari with Deep Reinforcement Learning}
\url{https://arxiv.org/abs/1312.5602}

\bibitem{gsurma_cartpole}
gsurma \\
\textit{OpenAI's cartpole env solver} \\
\url{https://github.com/gsurma/cartpole} 

\bibitem{learn_atari_ram}
Jakub Sygnowski, Henryk Michalewski \\
\textit{Learning from the memory of Atari 2600} \\
\url{https://arxiv.org/pdf/1605.01335v1.pdf} 

\bibitem{breakout_tkgw}
tkgw
\textit{algorithm on Breakout-ram-v0}
\url{https://gym.openai.com/evaluations/eval_tLiZx6dSwaX5YcVD4lrVg/}

\bibitem{maluuba}
\textit{Divide and conquer: How Microsoft researchers used AI to master Ms. Pac-Man} \newline
\url{https://blogs.microsoft.com/ai/divide-conquer-microsoft-researchers-used-ai-master-ms-pac-man/}

\bibitem{hra}
Harm van Seijen, Mehdi Fatemi, Joshua Romoff, Romain Laroche, Tavian Barnes, Jeffrey Tsang. \\
\textit{Hybrid Reward Architecture for Reinforcement Learning} \\
\url{https://arxiv.org/abs/1706.04208}

\bibitem{qlmaze}
Morvan Zhou \\
\textit{Q-learning maze} \\
\url{https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/2_Q_Learning_maze}

\bibitem{pacmanai1}
Sakchham Sharma \\
\textit{Deep Q Network optimized for mspacman environment} \\
\url{https://github.com/sakchhams/pacman_ai}

\bibitem{pacmanai2}
Chien-Sheng Wu \\
\textit{Artificial Intelligence, Pacman Game} \\
\url{https://github.com/jasonwu0731/AI-Pacman}

\bibitem{pacmanai3}
Xiao Ma \\
\textit{PacMan Machine Learning Artificial Intelligence Project} \\
\url{https://github.com/TuringKi/PacMan-AI}

\bibitem{dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov \\
\textit{Dropout: A Simple Way to Prevent Neural Networks from Overfitting} \\
\url{http://jmlr.org/papers/v15/srivastava14a.html}

\bibitem{gym_custom_env}
Martin Thoma
\url{https://stackoverflow.com/questions/45068568/how-to-create-a-new-gym-environment-in-openai}

\bibitem{exp_replay_deeper}
Shangtong Zhang, Richard S. Sutton \\
\textit{A Deeper Look at Experience Replay} \\
\url{https://arxiv.org/abs/1712.01275}

\bibitem{ai_faq}
Warren S. Sarle \\
\textit{comp.ai.neural-nets FAQ} \\
\url{http://www.faqs.org/faqs/ai-faq/neural-nets/part2/}


\end{thebibliography}